{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izeg8IkN08Sk"
   },
   "source": [
    "#Setup\n",
    "\n",
    "Install InterProt, load ESM and SAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJftHBAoVTc8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/etowahadams/interprot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "ba046a75ca9e4f88a4837e6967cb88b5",
      "b810c128662b4c0cbf1e3b0bbeb8c891",
      "39fcc44e30ea4e9a8f5dd9ca7a02367f",
      "ef12bac4667544f0a09549211646f401",
      "d18f723cf0244496865713d8958090b1",
      "47a6bcd7bb104a7cb2792e56c6e46ecb",
      "78ecae9dbaeb434ab7851ee4bceb5a1e",
      "260530d3247d4e8d8a6bc1fa23f33ee3",
      "420b35ed37c34b82abaed4645a723140",
      "3c02877ba2d04e5fbc76b25998fb80ff",
      "dba8a2d385d5490481b5e60e97dc511f"
     ]
    },
    "id": "F2pL5qwzJv-7",
    "outputId": "d9f05bc7-5bf6-49c2-b768-4b62c318ed36"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba046a75ca9e4f88a4837e6967cb88b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "esm2_plm1280_l24_sae4096.safetensors:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from transformers import AutoTokenizer, EsmForMaskedLM\n",
    "from safetensors.torch import load_file\n",
    "from interprot.sae_model import SparseAutoencoder\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ESM_DIM = 1280\n",
    "SAE_DIM = 4096\n",
    "LAYER = 24\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "esm_lm = EsmForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t33_650M_UR50D\")\n",
    "esm_lm.to(device).eval()\n",
    "\n",
    "# Load SAE model\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"liambai/InterProt-ESM2-SAEs\",\n",
    "    filename=\"esm2_plm1280_l24_sae4096.safetensors\"\n",
    ")\n",
    "sae_model = SparseAutoencoder(ESM_DIM, SAE_DIM)\n",
    "sae_model.load_state_dict(load_file(checkpoint_path))\n",
    "sae_model.to(device)\n",
    "sae_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrQ97aWukFvJ"
   },
   "source": [
    "#Inference\n",
    "\n",
    "\n",
    "First, get ESM layer 24 activations, encode it with SAE to get a (L, 4096) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjgljIpn5jRc",
    "outputId": "9efc4dd7-4a1c-4bed-a314-227667be58d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 1280])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"MATLFHDTSQSEENGSDDNLSLENEEKLKALGCREDPVNILVIGPAGAGKSTLINALFGKDVATVGYGARGVTTEIHSYEGEYKGVRIRVYDTVGFEGRSDWSYLRNIRRHEKYDLVLLCTKLGGRVDRDTFLELASVLHEEMWKKTIVVLTFANQFITLGSVAKSNDLEGEINKQIEEYKSYLTGRLSNCVRKEALVGIPFCIAGVEDERELPTTEDWVNTLWDKCIDRCSNETYHFASWFSIIKIVAFGFGVAIGTAIGAIVGSIVPVTGTIIGAIAGGYIGAAITKRVVEKFKNY\"\n",
    "\n",
    "inputs = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Forward pass → grab hidden_states[LAYER]\n",
    "with torch.no_grad():\n",
    "    out = esm_lm(**inputs,\n",
    "                 output_hidden_states=True,\n",
    "                 return_dict=True)\n",
    "\n",
    "# hidden_states is tuple(len=layers+1) of (batch, seq_len, hidden_dim)\n",
    "esm_layer_acts = out.hidden_states[LAYER]\n",
    "\n",
    "print(\"ESM layer-24 acts shape:\", esm_layer_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVp6gMzH7qH0",
    "outputId": "36d32894-9b35-41ba-9833-294a2aeaadca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE latents shape: torch.Size([300, 4096])\n"
     ]
    }
   ],
   "source": [
    "sae_latents, mu, std = sae_model.encode(esm_layer_acts[0])\n",
    "\n",
    "print(\"SAE latents shape:\", sae_latents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99CM7LtnkEMY"
   },
   "source": [
    "Decode the SAE latents yields a (L, 1280) tensor `decoded_esm_layer_acts`,\n",
    "i.e. the SAE's prediction of ESM layer 24 acts. This prediction can be off: compute the error as `recons_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFE3L-7Pe2PW",
    "outputId": "c58c3092-8527-4c2f-dbcb-824640a28b7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6259,  1.0401,  1.6099,  ..., -2.9674, -1.3321,  1.6541],\n",
       "         [-1.4183,  0.1804, -0.9507,  ..., -0.6793, -2.6099, -1.9202],\n",
       "         [ 0.0181,  5.0232,  3.1489,  ...,  0.4581, -0.1087,  1.5976],\n",
       "         ...,\n",
       "         [ 2.1382,  2.4558,  2.0668,  ...,  2.7426, -0.4519, -2.6419],\n",
       "         [ 3.0010,  1.8902,  5.3748,  ..., -1.5816,  1.0738, -5.1460],\n",
       "         [-0.2634, -1.1311, -1.2312,  ..., -4.4766,  2.6242, -0.4650]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_esm_layer_acts = sae_model.decode(sae_latents, mu, std)\n",
    "recons_error = esm_layer_acts - decoded_esm_layer_acts\n",
    "\n",
    "recons_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yE4n1fkhQu_"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(decoded_esm_layer_acts + recons_error, esm_layer_acts, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83g9SooAX74O"
   },
   "outputs": [],
   "source": [
    "max_act = sae_latents.max()\n",
    "sae_latents[:, 220] = max_act * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYASjoRZmTEq"
   },
   "source": [
    "Decode modified SAE latents back into ESM feature‐space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aat64sPtXXs-",
    "outputId": "ec8303e2-4b2c-459d-988a-63e6368067f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered sequence: MSSSSSSSSSSEESSSSSSSAEEAQAAAAALAEEAKKLKIVVVGASSAGKSTFINSTSGTTSASSSSSSTSSTTTTVTYVSSSSNKRVVLVDTVGVFDSEEALVLLVLLLIASVDLVLLLLLLLNSSSEVTVETFTTAFDAEAANRVVVVLNNCDEVNNEEEEENNNNEEEEENEEIEEIINIIITIITNNNNNNIIVNIIVVVVNNAAAATLLVTLLLDLLLVLLLLLLLAAAAAAQAAKIAIIAAAATAATTAAGAAAGAAAGALAPAAGAAAGAAAGGAAGAAAAKKVAKKKKKK\n"
     ]
    }
   ],
   "source": [
    "clamped = sae_model.decode(sae_latents, mu, std).unsqueeze(0)   # (1, L, 1280)\n",
    "\n",
    "# Get full‐vocab logits\n",
    "logits = esm_lm.lm_head(clamped + recons_error)                 # (1, L, vocab_size)\n",
    "\n",
    "# Mask out all special‐token IDs\n",
    "special_ids = {\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.mask_token_id,\n",
    "    tokenizer.unk_token_id,\n",
    "}\n",
    "for sid in special_ids:\n",
    "    logits[..., sid] = -1e9\n",
    "\n",
    "# Argmax over the remaining classes, skipping BoS/EoS\n",
    "tokens = torch.argmax(logits[:, 1:-1, :], dim=-1)               # (1, L-2)\n",
    "\n",
    "# Convert to letters\n",
    "steered_seq = \"\".join(tokenizer.convert_ids_to_tokens(tokens[0].tolist()))\n",
    "print(\"Steered sequence:\", steered_seq)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
