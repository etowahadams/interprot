{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izeg8IkN08Sk"
   },
   "source": [
    "#Setup\n",
    "\n",
    "Install InterProt, load ESM and SAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SJftHBAoVTc8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/etowahadams/interprot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283,
     "referenced_widgets": [
      "28dbd034f2fb490ea932b7edf7200804",
      "e546fc888a4b44a9a7a5cac08e75ba06",
      "c087c5b0ef444d718722618799db5165",
      "a6b7d57bcefe4ec596dd046c90c7d9d1",
      "4a0c2cf1faf540cba62f81167f59a88d",
      "fc0b1528078945ccab337f758fe51022",
      "e4fecdae4fd94d96a52d540dd50d2771",
      "91b75795f6914f5ea48c1e5367a60493",
      "a8ac3d35da154fd9b30a7cfb3dd54aa3",
      "2618916040304d83b9ce71177d0e34f2",
      "d17d04eb33814b47a9bbbaedd5580baf",
      "bf5c02a2b78644db9f889c31fc4910ae",
      "9d95d78e4892401488f45bd48ebf2621",
      "44746e016a334fe39c53c509ea0f0578",
      "79c8bac1a88d4d27823b0cc8aa596509",
      "220b4093813e4dbf861d83058fb4b902",
      "85fd513dc13b4ab6be5d5d6476572432",
      "505b070af37d401c877e70df31e31214",
      "857769068764402f8a9cfe19b889dd3f",
      "758c7ef6bc774db5a8147268f7d87789",
      "1add1b530b264b2f85c99de837fa79c0",
      "a5bdb9c2cb1e4609a57daebf5f142708",
      "859090881c63408f8b01b57c02218807",
      "dadaa43c657d46e08cee8ae651187120",
      "0a7450d68f764adca303223b881bb070",
      "852ef9ac8ddb4c41b5ce8bec410e8341",
      "3a173d92d61b44c2958e0c01a3572e85",
      "0979351e145549fcb6e97a3e0a533f91",
      "71e5b5dc14cb4cacb4b3c661785d5132",
      "d86634e0f687422e801342958632cb16",
      "41f6ccb096ee4d6f9a160789d9c3efb6",
      "4620ac3ac2aa41a8ae4186bda9ff1684",
      "17460c080dfe471ba938a5899b966b8f",
      "09a2ebd7df7b4e41b601fce0dd97d55d",
      "cacfc3747de04f79bb55984c0ce278b2",
      "73949136ec814e7b9f0bbd0347600d56",
      "47f9f2f6313a4de8860221bb39242e7b",
      "01879b65160c478bb79fa3b72f8c59eb",
      "6e0b70c9f63f4b7384a44cf2052a5e3d",
      "8540a69b969244a8a21a6a829b5c6de7",
      "03bf3cb551974a128057ccc079436b32",
      "8322bde84e3b460296f630579a995008",
      "8cd506c9d4774442963bde82444542a6",
      "2db531e925bb4bf1a90073dffe805893",
      "92c4c26350e34b7b93932e0fef262674",
      "45a0a49dd37e40979cf6ea6350447cec",
      "8729b17bba7b461b8fbb03ab8682a062",
      "bc22f1166c1c4a67bf17614bd101894f",
      "a385f5566bb54a2ea5810ff0a6e5a15f",
      "80d85f649c174d52b826da2bb6a9f98a",
      "02fb23cc0edb4e9e880eb25a2bb2e6d7",
      "83f753d27e22488a8a1053b97543274a",
      "66a1d64b62454af6a2d8452ce39bf477",
      "45fc8c306f3c439bbb4ef9d8b7db50b2",
      "4d104c50efb6415285faab11a7109b57",
      "e12aa99caa2c4dc18c04cd645982ce47",
      "81c936f2907e4b5daea656c78ef0e6b1",
      "e3925ea29c194740ac217bba6ccfcad3",
      "72d44a94566749abbca3b860db9a8138",
      "acb668444bcb4cdfb9d0cc6622bdfc7e",
      "4811dbfcb68e42a98bbce364a817dec4",
      "8298dd80b1094342aad53f506f5b02fb",
      "42648f8a4419491eb5038630b5a74323",
      "6dc40cb9d579468faedb0cf398218406",
      "61cce1d1aa2e4638a29a40dbd2278851",
      "d052584fa75541ceb704614f488511de"
     ]
    },
    "id": "F2pL5qwzJv-7",
    "outputId": "9402c002-f4e2-4412-8e7a-6462c8d16f54"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dbd034f2fb490ea932b7edf7200804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5c02a2b78644db9f889c31fc4910ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859090881c63408f8b01b57c02218807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a2ebd7df7b4e41b601fce0dd97d55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c4c26350e34b7b93932e0fef262674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12aa99caa2c4dc18c04cd645982ce47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)sm2_plm1280_l24_sae4096_100k.safetensors:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from transformers import AutoTokenizer, EsmForMaskedLM\n",
    "from safetensors.torch import load_file\n",
    "from interprot.sae_model import SparseAutoencoder\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ESM_DIM = 1280\n",
    "SAE_DIM = 4096\n",
    "LAYER = 24\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "esm_lm = EsmForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t33_650M_UR50D\")\n",
    "esm_lm.to(device).eval()\n",
    "\n",
    "# Load SAE model\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"liambai/InterProt-ESM2-SAEs\",\n",
    "    filename=\"esm2_plm1280_l24_sae4096_100k.safetensors\"\n",
    ")\n",
    "sae_model = SparseAutoencoder(ESM_DIM, SAE_DIM)\n",
    "sae_model.load_state_dict(load_file(checkpoint_path))\n",
    "sae_model.to(device)\n",
    "sae_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrQ97aWukFvJ"
   },
   "source": [
    "#Inference\n",
    "\n",
    "\n",
    "First, get ESM layer 24 activations, encode it with SAE to get a (L, 4096) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjgljIpn5jRc",
    "outputId": "b5c354af-8ec5-48ad-b6a1-dad33a0a1c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM layer-24 acts shape: torch.Size([1, 300, 1280])\n"
     ]
    }
   ],
   "source": [
    "seq = \"MATLFHDTSQSEENGSDDNLSLENEEKLKALGCREDPVNILVIGPAGAGKSTLINALFGKDVATVGYGARGVTTEIHSYEGEYKGVRIRVYDTVGFEGRSDWSYLRNIRRHEKYDLVLLCTKLGGRVDRDTFLELASVLHEEMWKKTIVVLTFANQFITLGSVAKSNDLEGEINKQIEEYKSYLTGRLSNCVRKEALVGIPFCIAGVEDERELPTTEDWVNTLWDKCIDRCSNETYHFASWFSIIKIVAFGFGVAIGTAIGAIVGSIVPVTGTIIGAIAGGYIGAAITKRVVEKFKNY\"\n",
    "\n",
    "inputs = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Forward pass → grab hidden_states[LAYER]\n",
    "with torch.no_grad():\n",
    "    out = esm_lm(**inputs,\n",
    "                 output_hidden_states=True,\n",
    "                 return_dict=True)\n",
    "\n",
    "# hidden_states is tuple(len=layers+1) of (batch, seq_len, hidden_dim)\n",
    "esm_layer_acts = out.hidden_states[LAYER]\n",
    "\n",
    "print(\"ESM layer-24 acts shape:\", esm_layer_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVp6gMzH7qH0",
    "outputId": "d981e2af-da2f-4bfb-e8da-083bd54fbe97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE latents shape: torch.Size([300, 4096])\n"
     ]
    }
   ],
   "source": [
    "sae_latents, mu, std = sae_model.encode(esm_layer_acts[0])\n",
    "\n",
    "print(\"SAE latents shape:\", sae_latents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99CM7LtnkEMY"
   },
   "source": [
    "Decoding the SAE latents yields a (L, 1280) tensor `decoded_esm_layer_acts`\n",
    "(i.e., the SAE's prediction of ESM layer 24 acts). This prediction can be off: compute the error as `recons_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFE3L-7Pe2PW",
    "outputId": "963e0074-be57-4353-9bbd-dd8df75e0b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4678,  0.7146,  3.8358,  ..., -1.6600, -0.6162, -1.5107],\n",
       "         [-1.4838,  0.7454,  1.7162,  ..., -2.4572, -4.2533, -4.9109],\n",
       "         [-1.5524,  2.9380,  1.2259,  ...,  1.1724, -0.8309, -2.2150],\n",
       "         ...,\n",
       "         [ 1.8651, -4.5614,  3.0894,  ...,  0.3076,  1.5975,  0.7138],\n",
       "         [ 4.4343, -3.4748,  2.7736,  ..., -3.1804, -0.1675,  0.2339],\n",
       "         [ 2.1849, -1.0958, -1.0139,  ..., -4.1312, -0.2550, -2.3275]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_esm_layer_acts = sae_model.decode(sae_latents, mu, std)\n",
    "recons_error = esm_layer_acts - decoded_esm_layer_acts\n",
    "\n",
    "recons_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7yE4n1fkhQu_"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(decoded_esm_layer_acts + recons_error, esm_layer_acts, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "83g9SooAX74O"
   },
   "outputs": [],
   "source": [
    "max_act = sae_latents.max()\n",
    "sae_latents[:, 220] = max_act * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYASjoRZmTEq"
   },
   "source": [
    "Decode modified SAE latents back into ESM feature‐space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aat64sPtXXs-",
    "outputId": "05f24032-dc8a-4105-a474-6ce896077287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered sequence: TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n"
     ]
    }
   ],
   "source": [
    "clamped = sae_model.decode(sae_latents, mu, std).unsqueeze(0)   # (1, L, 1280)\n",
    "\n",
    "# Get full‐vocab logits\n",
    "logits = esm_lm.lm_head(clamped + recons_error)                 # (1, L, vocab_size)\n",
    "\n",
    "# Mask out all special‐token IDs\n",
    "special_ids = {\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.mask_token_id,\n",
    "    tokenizer.unk_token_id,\n",
    "}\n",
    "for sid in special_ids:\n",
    "    logits[..., sid] = -1e9\n",
    "\n",
    "# Argmax over the remaining classes, skipping BoS/EoS\n",
    "tokens = torch.argmax(logits[:, 1:-1, :], dim=-1)               # (1, L-2)\n",
    "\n",
    "# Convert to letters\n",
    "steered_seq = \"\".join(tokenizer.convert_ids_to_tokens(tokens[0].tolist()))\n",
    "print(\"Steered sequence:\", steered_seq)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
